{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NourEldin-Osama/T5_Fine-tuning_Text-simplification/blob/main/Notebooks/Text_simplification_Client.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9mWsHy31y_D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install fastapi pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOtM9QMNp65I",
        "outputId": "2b60177b-35be-4eaa-a504-0be0f3368dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "class TextSimplificationModel:\n",
        "    def __init__(self, tokenizer, model):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def simplify(self, text):\n",
        "        text = \"simplify: \" + text\n",
        "        encoded_input = self.tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "        outputs = self.model.generate(encoded_input)\n",
        "        outputs = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1gb2w0xpz2b",
        "outputId": "ef3aeffe-83d0-4e7b-8161-b6e2aeef1193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from typing import Union\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from utils import TextSimplificationModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NourEldin-Osama/t5-small-finetuned-text-simplification\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"NourEldin-Osama/t5-small-finetuned-text-simplification\")\n",
        "\n",
        "app = FastAPI()\n",
        "Model = TextSimplificationModel(tokenizer, model)\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "class UserInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get('/')\n",
        "def root():\n",
        "    print(\"root() called\")\n",
        "    return {'message': 'API is Working ðŸš€'}\n",
        "\n",
        "\n",
        "@app.post(\"/text_simplification/\")\n",
        "def text_simplification(request: UserInput):\n",
        "    text = request.text\n",
        "    output = Model.simplify(text)\n",
        "    return {\"output\": output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPEwmJm-12o8",
        "outputId": "4abbf5a2-8a83-4e60-dea1-2ab0fdf60793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-04T19:17:56+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://6e7d-104-197-81-116.ngrok.io\n",
            "Server Link: https://6e7d-104-197-81-116.ngrok.io/text_simplification/\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "url = ngrok_tunnel.public_url\n",
        "print('Public URL:', url)\n",
        "print(f'Server Link: {url}' + r'/text_simplification/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOW2RHTH2ApB",
        "outputId": "bd77c70f-48f7-4cef-9f2f-3eed62e582dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m528\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "Downloading (â€¦)okenizer_config.json: 100% 2.35k/2.35k [00:00<00:00, 11.8MB/s]\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 15.3MB/s]\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 19.3MB/s]\n",
            "Downloading (â€¦)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 11.2MB/s]\n",
            "Downloading (â€¦)lve/main/config.json: 100% 1.47k/1.47k [00:00<00:00, 6.57MB/s]\n",
            "Downloading pytorch_model.bin: 100% 242M/242M [00:02<00:00, 118MB/s]\n",
            "Downloading (â€¦)neration_config.json: 100% 142/142 [00:00<00:00, 540kB/s]\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m531\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mGET /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[31m405 Method Not Allowed\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.47.176.25:0 - \"\u001b[1mGET /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[31m405 Method Not Allowed\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.47.176.25:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "2023-05-04 19:23:19.279477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     105.32.77.69:0 - \"\u001b[1mPOST /text_simplification/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uvicorn main:app --reload --host 127.0.0.1 --port 8000"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhgEA2tt4JL9RGTIJUDA8P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}